---
title: "Prediction of Exercise Motion Type"
output: html_document
---
```{r, echo = FALSE, warning = FALSE, message = FALSE}
set.seed(1)
library(lattice)
library(ggplot2)
library(caret)
library(randomForest)
```

###Summary
The purpose of this project is to be able to identify motion errors an individual can make while performing the Unilateral Dumbbell Biceps Curl exercise. The data for this project comes from "Qualitative Activity Recognition of Weight Lifting Exercises" by Velloso, et. al. In their study they attached several sensors to a number of men and instructed them to perform the biceps curl exercise, while instructing them to make certain qualitative errors in the method they perform the exercise, for instance "throwing hips to the front." They took motion sensor measurements from several places on the individuals' bodies throughout the exercise, and classified the data according to the motion they had instructed them to make. There are five different types of motions they were instructed to make: the correct motion and four different types of errors.

###Data Contents
The data set consists of: the participant's name, a time stamp, a variable indicating whether an observation is a measurement taken at the start of a new exercise or during an exercise, the raw data taken by the sensors, statistical summary variables (eg. mean, skewness, kurtosis) taken by the sensors over each full exercise motion, and a variable "classe" that indicates what motion type the individual was instructed to do.

```{r, echo = FALSE}
raw.data <- read.csv("pml-training.csv")
#str(raw.data)
```

###Data Processing
In their paper Velloso, et. al. build a model to predict the classe variable using the statistical summary measurements for each complete exercise as features. However, in the testing data set provided for the course project the raw sensor measurements are provided, but not the statistical summary data. Thus we must build a model using the raw measurements as features.

Since there is only one summary measurement for each exercise, and each exercise may span hundreds of observations, the summary measurement data is spare by definition. Therefore, I started by removing features from the dataset that had a majority of the values missing.

I also then removed the data containing the participant's name and the measurement time stamp. Although the testing samples are drawn from the same data set, so including these variables would improve the model accuracy on the data set, a model including them would lack generality to new measurements and be a somewhat trivial solution and I wanted to first attempt building a model that could generalize.

```{r}
majorityMissing <- function(col) {
      n.missing <- sum(sapply(col, function(x) x == "" || is.na(x)))
      percent.missing <- n.missing/length(col)
      
      if (percent.missing > 0.5) {
            return(TRUE)
      } else {
            return(FALSE)
      }
}

to.remove <- c()

for (i in 1:ncol(raw.data)){
      if (majorityMissing(raw.data[,i])) {
            to.remove <- c(to.remove, i)     
      }
}

to.remove <- c(1:7, to.remove)

activity <- raw.data[,-to.remove]
```

###Model Building and Cross Validation
I started by dividing the data set into 3 groups: a Training set, and a Test set with 60%, 40% respectively. I opted to use a Random Forest model and my approach was to use k-folds cross validation on the Training set to estimate the out of sample error rate, then train a random forest model on the entire training set and use the test set to verify the out of sample error rate.

```{r}

in.train = createDataPartition(y = activity$classe, p = 0.6, list = FALSE)
training <- activity[in.train, ]
test <- activity[-in.train, ]

```

I started by creating 5 k-fold splits of the Training data into k training and test sets, then training 5 random forest models on the training sets and checking the out of sample error rate on the k test sets.

```{r}
train.folds <- createFolds(y = training$classe, k = 5, list = TRUE, returnTrain = TRUE)

error.rates <- c()

for (i in 1:length(train.folds)) {
      k.train <- training[train.folds[[i]],]
      k.test <- training[-train.folds[[i]],]
      
      k.model <- randomForest(y = k.train$classe, x = k.train[, -ncol(k.train)])
      k.preds <- predict(k.model, k.test[,-ncol(k.test)])
      error.rates <- c(error.rates, 1 - sum(k.test$classe == k.preds) / 
                                 length(k.test$classe))
}

summary(error.rates)

```

The average error rate in the k-folds cross validation is 0.93%. Now I train the random forest model on the entire Training data set, predict values for the Testing data, and check the error rate.

```{r}
model <- randomForest(y = training$classe, x = training[, -ncol(training)])

preds <- predict(model, test[,-ncol(test)])

confusionMatrix(test$classe, preds)$overall
```


The model achieves an out of sample error rate of 0.72% with a [0.54%, 0.93%] 95% confidence interval on the Testing set which lines up with the result from the k-folds cross validation estimate.
